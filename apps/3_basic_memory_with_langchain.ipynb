{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ad515b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f3fe42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914d1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac0707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"user\", \"content\" : \"My name is Joe and I am disgusting\"},\n",
    "    {\"role\": \"ai\", \"content\": \"Joe, it sounds like you're going through a really tough time and are feeling incredibly negative about yourself right now. I want you to know that feeling this way doesn't make it true. Everyone has inherent worth, and it's common to struggle with self-perception, especially during difficult periods.\\n\\nIf you're feeling this way, it might be helpful to talk to someone who can offer support. Would you like to share more about what's making you feel this way, or would you be interested in information about resources that can help you work through these feelings?\"},\n",
    "    {\"role\": \"user\", \"content\" : \"What is my name?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55f4baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77ff845f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Joe.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29c3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.invoke('what is my name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0078f453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know your name. As an AI, I don't have personal memory of who you are or access to your personal information.\\n\\nYou haven't told me your name in this conversation. If you'd like me to know it for our current chat, please feel free to tell me!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368cf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76a4ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## manage history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78295027",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d5eb361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:  hi\n",
      "ai: Hi there! How can I help you today?\n",
      "user:  i am a data databricks engineer\n",
      "ai: That's fantastic! Databricks is a powerful and rapidly evolving platform, and engineers specializing in it are in high demand.\n",
      "\n",
      "What brings you to me today, or what's on your mind regarding your work as a Databricks Engineer?\n",
      "\n",
      "I can assist with a wide range of topics, such as:\n",
      "\n",
      "*   **Spark Optimization:** Performance tuning, understanding execution plans, optimizing dataframes.\n",
      "*   **Delta Lake:** Best practices for ACID transactions, schema evolution, time travel, Z-ordering, liquid clustering.\n",
      "*   **Data Engineering Workflows:** Designing ETL/ELT pipelines, using Databricks Lakehouse Platform features like DLT (Delta Live Tables).\n",
      "*   **MLflow & MLOps:** Tracking experiments, model management, deployment strategies on Databricks.\n",
      "*   **Unity Catalog:** Implementing data governance, security, and access control.\n",
      "*   **Troubleshooting:** Debugging failed jobs, performance bottlenecks.\n",
      "*   **Architecture Design:** Best practices for structuring your Lakehouse, data partitioning strategies.\n",
      "*   **Coding Assistance:** Python, Scala, SQL, PySpark/Spark SQL queries.\n",
      "*   **Databricks Features:** Notebooks, Repos, Jobs, Workflows, Clusters, Photon.\n",
      "*   **Cost Optimization:** Strategies to reduce cluster costs and optimize resource usage.\n",
      "*   **CI/CD & DevOps:** Integrating Databricks with modern development practices.\n",
      "*   **New Features & Best Practices:** Staying up-to-date with the latest from Databricks.\n",
      "\n",
      "So, whether you have a specific problem you're trying to solve, want to discuss best practices, or are just curious about a particular Databricks feature, feel free to dive right in!\n",
      "user:  how to take care of optimization\n",
      "ai: Optimization in Databricks is a continuous and multi-faceted process that spans data storage, compute resources, code efficiency, and workflow design. As a Databricks Engineer, mastering these aspects is crucial for cost-effectiveness, performance, and scalability.\n",
      "\n",
      "Here's a comprehensive guide on how to approach optimization:\n",
      "\n",
      "---\n",
      "\n",
      "### I. Data Layer Optimization (Delta Lake Best Practices)\n",
      "\n",
      "The foundation of your Lakehouse is Delta Lake, and optimizing your data here pays huge dividends.\n",
      "\n",
      "1.  **Small File Problem & Compaction (`OPTIMIZE`)**:\n",
      "    *   **Issue:** Many small files lead to high metadata overhead, slower reads, and inefficient resource utilization.\n",
      "    *   **Solution:** Regularly run `OPTIMIZE` on your Delta tables.\n",
      "        ```sql\n",
      "        OPTIMIZE delta.`/path/to/table`\n",
      "        -- OR\n",
      "        OPTIMIZE my_delta_table\n",
      "        ```\n",
      "    *   **Recommendation:** Schedule `OPTIMIZE` as a recurring job, especially after ingestions or `MERGE` operations. Databricks often does some auto-compaction, but explicit `OPTIMIZE` is still valuable.\n",
      "    *   **Target File Size:** Databricks generally aims for ~128MB-1GB files. You can tune this with `delta.targetFileSize` table property.\n",
      "\n",
      "2.  **Data Skipping & Indexing (`ZORDER`, `LIQUID CLUSTERING`)**:\n",
      "    *   **Issue:** Scans too much data to find relevant rows.\n",
      "    *   **Solution:**\n",
      "        *   **`ZORDER` (Older, still useful):** Collocate related information in the same set of files. Best for columns frequently used in `WHERE` clauses and joins.\n",
      "            ```sql\n",
      "            OPTIMIZE my_delta_table ZORDER BY (column1, column2)\n",
      "            ```\n",
      "            *   **Recommendation:** Apply `ZORDER` on high-cardinality columns that are frequently filtered or joined.\n",
      "        *   **`LIQUID CLUSTERING` (Newer, Recommended):** A more flexible and performant alternative to partitioning and `ZORDER`. It automatically manages data layout based on query patterns.\n",
      "            ```sql\n",
      "            CREATE TABLE my_table (col1 INT, col2 STRING) USING DELTA CLUSTER BY (col1);\n",
      "            ALTER TABLE my_table CLUSTER BY (col1, col2);\n",
      "            ```\n",
      "            *   **Recommendation:** Use `LIQUID CLUSTERING` for new tables or migrate existing ones where applicable. It's often \"set and forget\" and adapts better.\n",
      "\n",
      "3.  **Partitioning (Legacy/Specific Use Cases)**:\n",
      "    *   **Issue:** Over-partitioning or under-partitioning can hurt performance.\n",
      "    *   **Solution:** Use partitioning for columns with *low cardinality* that are frequently filtered, especially when combined with `OPTIMIZE` and `ZORDER`.\n",
      "    *   **Recommendation:** With `LIQUID CLUSTERING` becoming the norm, traditional partitioning is less critical. Avoid deep partitioning trees (e.g., `year/month/day/hour`).\n",
      "\n",
      "4.  **Schema Evolution & Enforcement**:\n",
      "    *   **Issue:** Inconsistent schemas can lead to data quality issues and failed jobs.\n",
      "    *   **Solution:** Leverage Delta Lake's schema evolution capabilities (`mergeSchema=true`) and schema enforcement. Prefer `MERGE` operations for upserts rather than full rewrites.\n",
      "\n",
      "5.  **Caching (Less common with Photon, but good to know)**:\n",
      "    *   **Issue:** Repeated reads of the same data from disk.\n",
      "    *   **Solution:** You can cache tables or specific queries in memory, though Databricks' Photon engine often makes explicit caching less necessary for many workloads.\n",
      "        ```sql\n",
      "        CACHE TABLE my_delta_table\n",
      "        ```\n",
      "    *   **Recommendation:** Consider for iterative machine learning workloads or highly repetitive BI queries if Photon doesn't fully address performance.\n",
      "\n",
      "---\n",
      "\n",
      "### II. Compute Layer Optimization (Spark Clusters & Configuration)\n",
      "\n",
      "Optimizing your compute resources is about getting the most bang for your buck.\n",
      "\n",
      "1.  **Photon Engine**:\n",
      "    *   **Issue:** Traditional Spark engine can be slower for certain workloads.\n",
      "    *   **Solution:** Enable Photon on your clusters. It's a vectorized query engine that runs SQL and DataFrame operations significantly faster.\n",
      "    *   **Recommendation:** Always use Photon-enabled clusters where possible. It's often a \"free\" performance boost.\n",
      "\n",
      "2.  **Cluster Sizing & Instance Types**:\n",
      "    *   **Issue:** Under-provisioned clusters lead to slow jobs; over-provisioned clusters waste money.\n",
      "    *   **Solution:**\n",
      "        *   **Auto-scaling:** Enable auto-scaling for both workers and local storage.\n",
      "        *   **Instance Types:** Choose instance types appropriate for your workload (e.g., memory-optimized for joins/aggregations, compute-optimized for CPU-bound tasks).\n",
      "        *   **Driver Node:** Ensure your driver node is powerful enough, especially if you're collecting results or performing operations that aren't distributed.\n",
      "    *   **Recommendation:** Start with a reasonable size, monitor Spark UI, and adjust. Use Databricks' \"Enhanced Metrics\" for deeper insights.\n",
      "\n",
      "3.  **Spark Configuration**:\n",
      "    *   **Issue:** Default Spark configs aren't always optimal for specific workloads.\n",
      "    *   **Solution:** Tune Spark properties at the cluster, notebook, or job level.\n",
      "        *   `spark.sql.shuffle.partitions`: Adjust based on data size to avoid too many or too few shuffle files. (Commonly tuned)\n",
      "        *   `spark.databricks.io.cache.enabled`: Enable for faster reads if Photon isn't fully covering your use case.\n",
      "        *   `spark.driver.memory`, `spark.executor.memory`, `spark.executor.cores`: Adjust based on workload and instance types.\n",
      "    *   **Recommendation:** Only tune what you understand and have measured. Default settings with Photon are often a good starting point.\n",
      "\n",
      "4.  **Databricks Runtime (DBR) Version**:\n",
      "    *   **Issue:** Older runtimes might lack the latest performance improvements.\n",
      "    *   **Solution:** Always use the latest stable DBR version. Each release often includes performance enhancements, bug fixes, and new features.\n",
      "    *   **Recommendation:** Stay current.\n",
      "\n",
      "---\n",
      "\n",
      "### III. Code Optimization (Spark SQL & PySpark)\n",
      "\n",
      "The way you write your code profoundly impacts performance.\n",
      "\n",
      "1.  **Predicate Pushdown & Column Pruning**:\n",
      "    *   **Issue:** Reading more data than necessary.\n",
      "    *   **Solution:** Spark automatically tries to push down filters and select only required columns. Ensure your queries allow this.\n",
      "        *   Filter early (`WHERE` clauses).\n",
      "        *   Select only the columns you need (`SELECT column1, column2`).\n",
      "    *   **Recommendation:** Always apply filters as early as possible in your queries.\n",
      "\n",
      "2.  **Join Strategy Optimization**:\n",
      "    *   **Issue:** Inefficient joins can lead to massive data shuffling.\n",
      "    *   **Solution:**\n",
      "        *   **Broadcast Joins:** If one table is small (e.g., < 100MB), broadcast it to all executors. Spark often does this automatically (`spark.sql.autoBroadcastJoinThreshold`), but you can hint:\n",
      "            ```sql\n",
      "            SELECT /*+ BROADCAST(small_table) */ a.*, b.value FROM large_table a JOIN small_table b ON a.key = b.key;\n",
      "            ```\n",
      "        *   **Sort-Merge Joins / Shuffle Hash Joins:** For larger tables, ensure keys are well-distributed to avoid data skew.\n",
      "    *   **Recommendation:** Understand your data sizes and let Spark optimize where possible, but be ready to hint for broadcast joins.\n",
      "\n",
      "3.  **Avoid UDFs (User-Defined Functions) where possible**:\n",
      "    *   **Issue:** Python/Scala UDFs are black boxes to Spark's optimizer and can prevent many optimizations.\n",
      "    *   **Solution:** Prefer native Spark functions (e.g., `pyspark.sql.functions` or SQL functions) which are optimized and often run on Photon.\n",
      "    *   **Recommendation:** If you must use UDFs, consider Pandas UDFs (Vectorized UDFs) for PySpark, which are more performant for certain operations.\n",
      "\n",
      "4.  **Minimize Shuffles**:\n",
      "    *   **Issue:** Shuffling data across the network is expensive. Operations like `groupBy`, `join`, `repartition`, `orderBy` cause shuffles.\n",
      "    *   **Solution:**\n",
      "        *   Combine operations to reduce intermediate shuffles.\n",
      "        *   Use `repartition` judiciously (only when necessary for downstream operations).\n",
      "        *   Be mindful of wide transformations.\n",
      "    *   **Recommendation:** Check your Spark UI for stages with significant shuffle reads/writes.\n",
      "\n",
      "5.  **Use `DataFrame` API over `RDD` API**:\n",
      "    *   **Issue:** `RDD`s don't benefit from Spark's Catalyst Optimizer.\n",
      "    *   **Solution:** Always prefer `DataFrame` or `Spark SQL` for structured data processing.\n",
      "    *   **Recommendation:** Stick to DataFrames/Spark SQL unless you have a very specific, low-level RDD use case.\n",
      "\n",
      "6.  **Avoid `collect()`, `toPandas()`, `count()` on large datasets**:\n",
      "    *   **Issue:** These operations bring all data to the driver node, which can crash the driver or cause OOM errors.\n",
      "    *   **Solution:** Process data in a distributed manner. If you need a sample, use `limit()` before `collect()`.\n",
      "    *   **Recommendation:** Only use these for small results or for quick checks on sampled data.\n",
      "\n",
      "---\n",
      "\n",
      "### IV. Workflow & Job Optimization\n",
      "\n",
      "How you schedule and orchestrate your data pipelines.\n",
      "\n",
      "1.  **Delta Live Tables (DLT)**:\n",
      "    *   **Issue:** Manually managing dependencies, retries, error handling, and incremental processing can be complex.\n",
      "    *   **Solution:** Use DLT for building reliable, maintainable, and testable data pipelines. DLT automatically handles many optimizations like compaction, auto-scaling, and incremental data processing.\n",
      "    *   **Recommendation:** Adopt DLT for new ETL/ELT pipelines, especially for streaming or near real-time use cases.\n",
      "\n",
      "2.  **Incremental Processing**:\n",
      "    *   **Issue:** Reprocessing all data every time is inefficient.\n",
      "    *   **Solution:** Only process new or changed data. Delta Lake's `MERGE` and DLT's `APPLY CHANGES INTO` are great for this.\n",
      "    *   **Recommendation:** Design pipelines to be incremental from the start.\n",
      "\n",
      "3.  **Job Scheduling & Concurrency**:\n",
      "    *   **Issue:** Contention for resources (e.g., multiple jobs writing to the same table, or overloading a cluster).\n",
      "    *   **Solution:**\n",
      "        *   Schedule jobs during off-peak hours.\n",
      "        *   Use separate clusters for different workloads (e.g., interactive development vs. production jobs).\n",
      "        *   Leverage Databricks Workflows for orchestrating complex DAGs of jobs.\n",
      "    *   **Recommendation:** Plan your job execution carefully.\n",
      "\n",
      "4.  **Consider Databricks SQL Warehouses**:\n",
      "    *   **Issue:** Running BI/ad-hoc queries on general-purpose Spark clusters can be expensive and slow.\n",
      "    *   **Solution:** Use Databricks SQL Warehouses (formerly SQL Endpoints). They are highly optimized for SQL queries, auto-scale rapidly, and provide superior performance for BI workloads.\n",
      "    *   **Recommendation:** Route all your BI and ad-hoc SQL queries through SQL Warehouses.\n",
      "\n",
      "---\n",
      "\n",
      "### V. Monitoring & Troubleshooting\n",
      "\n",
      "You can't optimize what you don't measure.\n",
      "\n",
      "1.  **Spark UI**:\n",
      "    *   **Tool:** The primary tool for understanding job execution, identifying bottlenecks, and analyzing stages, tasks, and shuffles.\n",
      "    *   **Focus:** Look for skewed tasks, long-running stages, large shuffle sizes, garbage collection issues.\n",
      "    *   **Recommendation:** Learn to read the Spark UI proficiently.\n",
      "\n",
      "2.  **Databricks Query History**:\n",
      "    *   **Tool:** Provides a history of all queries run, execution times, and associated cluster details.\n",
      "    *   **Focus:** Identify long-running queries, frequent queries that could be optimized or materialized.\n",
      "\n",
      "3.  **`EXPLAIN` Plans**:\n",
      "    *   **Tool:** Use `df.explain()` or `EXPLAIN SELECT ...` to see the logical and physical execution plan of your Spark queries.\n",
      "    *   **Focus:** Understand how Spark is transforming and executing your data. Look for unexpected shuffles, full table scans, or non-optimal join strategies.\n",
      "\n",
      "4.  **Databricks Logs & Metrics**:\n",
      "    *   **Tool:** Cluster logs and Databricks' enhanced metrics provide insights into cluster health, resource utilization, and potential issues.\n",
      "    *   **Focus:** CPU usage, memory usage, disk I/O, network I/O.\n",
      "\n",
      "---\n",
      "\n",
      "### VI. Cost Optimization (Closely related to performance)\n",
      "\n",
      "Many performance optimizations directly lead to cost savings.\n",
      "\n",
      "1.  **Auto-scaling Clusters**: Ensures you're only paying for the compute you need.\n",
      "2.  **Spot Instances**: Utilize spot instances for non-critical or fault-tolerant workloads to significantly reduce costs.\n",
      "3.  **Cluster Policy**: Enforce specific instance types and configurations to prevent users from provisioning overly expensive clusters.\n",
      "4.  **Idle Cluster Termination**: Configure clusters to terminate after a period of inactivity.\n",
      "5.  **Efficient Code**: Faster code means less cluster time, which means less cost.\n",
      "\n",
      "---\n",
      "\n",
      "### Key Takeaways for a Databricks Engineer:\n",
      "\n",
      "*   **Start with the Data:** Optimize your Delta tables first (OPTIMIZE, ZORDER/LIQUID CLUSTERING).\n",
      "*   **Leverage Databricks Features:** Photon, DLT, and SQL Warehouses are designed for performance. Use them!\n",
      "*   **Write Efficient Code:** Prefer native Spark functions, understand join strategies, and minimize shuffles.\n",
      "*   **Monitor Relentlessly:** Use the Spark UI and `EXPLAIN` to understand what's happening under the hood.\n",
      "*   **Iterate and Test:** Optimization is not a one-time task. Implement a change, measure its impact, and refine.\n",
      "\n",
      "By systematically addressing these areas, you'll be well-equipped to \"take care of optimization\" effectively in your Databricks environment. What specific area are you most interested in diving deeper into?\n",
      "user:  optimize\n",
      "ai: Okay, let's distill the essence of \"optimize\" in Databricks into the most impactful and actionable areas.\n",
      "\n",
      "At its core, **optimization in Databricks** means ensuring your data pipelines, queries, and resource usage are as **fast, efficient, and cost-effective** as possible.\n",
      "\n",
      "Here are the absolute key pillars to focus on:\n",
      "\n",
      "1.  **Optimize Your Data Layer (Delta Lake is Key):**\n",
      "    *   **Compaction:** Regularly run `OPTIMIZE` on your Delta tables to combine small files into larger, more efficient ones. This is fundamental.\n",
      "    *   **Data Skipping/Indexing:** Use `ZORDER BY` (for specific columns) or, even better, **`LIQUID CLUSTERING`** (for new tables or migrations) to physically co-locate related data, allowing Spark to skip reading irrelevant files during queries.\n",
      "    *   **Partitioning (Carefully):** Use it sparingly for *low-cardinality* columns if `LIQUID CLUSTERING` isn't an option or suitable. Avoid over-partitioning.\n",
      "    *   **Incremental Processing:** Design your pipelines to only process new or changed data using `MERGE INTO` or Delta Live Tables.\n",
      "\n",
      "2.  **Optimize Your Compute Layer (Clusters & Runtime):**\n",
      "    *   **Photon Engine:** **Always use Photon-enabled clusters.** It's a vectorized query engine that significantly speeds up SQL and DataFrame operations.\n",
      "    *   **Cluster Sizing:** Use **auto-scaling** to match compute resources to your workload. Don't over-provision, but also don't under-provision (which leads to slow jobs).\n",
      "    *   **Databricks Runtime (DBR):** Stay on the **latest stable DBR** for the newest performance improvements and features.\n",
      "    *   **Spark Configurations:** Tune `spark.sql.shuffle.partitions` and other specific configs only when you've identified a bottleneck and understand the impact. Photon and DBR defaults are often good.\n",
      "\n",
      "3.  **Optimize Your Code (Spark SQL & PySpark):**\n",
      "    *   **Predicate Pushdown & Column Pruning:** Filter early and select only the columns you need. Spark does this automatically, but your query structure should enable it.\n",
      "    *   **Minimize Shuffles:** Operations like `GROUP BY`, `JOIN`, `ORDER BY`, `REPARTITION` cause expensive data movement. Structure your code to reduce unnecessary shuffles.\n",
      "    *   **Avoid UDFs (User-Defined Functions):** Prefer native Spark functions (`pyspark.sql.functions` or SQL functions) over Python/Scala UDFs, as UDFs bypass the Spark optimizer. If you must use UDFs, consider Pandas UDFs.\n",
      "    *   **Efficient Joins:** Understand your data sizes and use broadcast joins for small tables when appropriate (Spark often auto-broadcasts).\n",
      "    *   **Use DataFrame API:** Always prefer DataFrames/Spark SQL over the lower-level RDD API for structured data.\n",
      "\n",
      "4.  **Optimize Your Workflows & Architecture:**\n",
      "    *   **Delta Live Tables (DLT):** Leverage DLT for building robust, self-optimizing, and incremental ETL/ELT pipelines. It automates many of the above optimizations.\n",
      "    *   **Databricks SQL Warehouses:** For BI, reporting, and ad-hoc SQL queries, use SQL Warehouses. They are specifically optimized for SQL and offer superior performance and cost efficiency for those workloads.\n",
      "    *   **Job Scheduling:** Schedule jobs efficiently, considering concurrency and resource contention.\n",
      "\n",
      "5.  **Monitor & Profile:**\n",
      "    *   **Spark UI:** Your best friend for diagnosing performance issues. Learn to read it to identify skewed tasks, long-running stages, and shuffle bottlenecks.\n",
      "    *   **`EXPLAIN` Plans:** Use `df.explain()` or `EXPLAIN SELECT ...` to understand how Spark is planning to execute your query.\n",
      "    *   **Databricks Query History:** Review execution times to spot trends and identify areas for improvement.\n",
      "\n",
      "**In essence, \"optimize\" in Databricks means:**\n",
      "*   **Let Delta Lake help you:** Keep your data tidy and indexed.\n",
      "*   **Let Databricks help you:** Use Photon, DLT, and SQL Warehouses.\n",
      "*   **Help yourself:** Write efficient Spark code and monitor its performance.\n",
      "\n",
      "Which of these areas would you like to dive into more deeply? Or perhaps you have a specific optimization challenge in mind?\n",
      "user:  bye thanks\n",
      "ai: You're very welcome! Glad I could help.\n",
      "\n",
      "Feel free to reach out anytime if you have more questions or need further assistance with your Databricks engineering work.\n",
      "\n",
      "Have a great day!\n",
      "Good Bye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"user: \")\n",
    "    if query.lower() in ['exit', 'quit', 'bye']:\n",
    "        print(\"Good Bye!\")\n",
    "        break\n",
    "    history.append({\"role\": \"user\", \"content\": query})\n",
    "    print(\"user: \", query)\n",
    "    res = llm.invoke(history)\n",
    "    print(\"ai:\", res.content)\n",
    "    history.append({\"role\": \"ai\", \"content\" : res.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60e97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-series (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
